{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (1.22.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/evilshadow/.local/lib/python3.10/site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: requests in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (2.25.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests->transformers) (1.26.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests->transformers) (2020.12.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: peft in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from peft) (1.22.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from peft) (21.3)\n",
            "Requirement already satisfied: psutil in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from peft) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from peft) (6.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from peft) (2.0.1)\n",
            "Requirement already satisfied: transformers in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from peft) (4.32.0)\n",
            "Requirement already satisfied: tqdm in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from peft) (4.65.0)\n",
            "Requirement already satisfied: accelerate in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from peft) (0.22.0)\n",
            "Requirement already satisfied: safetensors in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from peft) (0.3.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.8)\n",
            "Requirement already satisfied: filelock in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.6.3)\n",
            "Requirement already satisfied: sympy in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers->peft) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/evilshadow/.local/lib/python3.10/site-packages (from transformers->peft) (2022.3.15)\n",
            "Requirement already satisfied: requests in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers->peft) (2.25.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers->peft) (0.13.3)\n",
            "Requirement already satisfied: fsspec in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->peft) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests->transformers->peft) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests->transformers->peft) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests->transformers->peft) (2020.12.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-2.0.1-cp310-none-macosx_11_0_arm64.whl (55.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (4.32.0)\n",
            "Requirement already satisfied: huggingface_hub in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (0.16.4)\n",
            "Collecting peft\n",
            "  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\n",
            "  Downloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: filelock in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch) (4.6.3)\n",
            "Collecting sympy (from torch)\n",
            "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting networkx (from torch)\n",
            "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "Requirement already satisfied: jinja2 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch) (3.1.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (1.22.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/evilshadow/.local/lib/python3.10/site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: requests in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (2.25.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: psutil in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from peft) (5.9.0)\n",
            "Collecting accelerate (from peft)\n",
            "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/4d/a7/05c67003d659a0035f2b3a8cf389c1d9645865aee84a73ce99ddab16682f/accelerate-0.22.0-py3-none-any.whl.metadata\n",
            "  Downloading accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests->transformers) (1.26.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests->transformers) (2020.12.5)\n",
            "Collecting mpmath>=0.19 (from sympy->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, sympy, networkx, torch, accelerate, peft\n",
            "Successfully installed accelerate-0.22.0 mpmath-1.3.0 networkx-3.1 peft-0.5.0 sympy-1.12 torch-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers huggingface_hub peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-08-25T09:20:38.8179595Z",
              "execution_start_time": "2023-08-25T09:20:21.1401649Z",
              "livy_statement_state": "available",
              "parent_msg_id": "b0604fc0-761c-4a4e-9a3a-baab8d00c83d",
              "queued_time": "2023-08-25T09:17:09.2351799Z",
              "session_id": "0",
              "session_start_time": "2023-08-25T09:17:09.2365077Z",
              "spark_jobs": {
                "jobs": [],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "b3867972-38d7-400b-bd4e-75a3e99d299f",
              "state": "finished",
              "statement_id": 6
            },
            "text/plain": [
              "StatementMeta(b3867972-38d7-400b-bd4e-75a3e99d299f, 0, 6, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 744 kB/s eta 0:00:01\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-08-25T09:26:03.0534585Z",
              "execution_start_time": "2023-08-25T09:20:38.8880776Z",
              "livy_statement_state": "available",
              "parent_msg_id": "8f83178d-e8a3-44a5-89fb-f23651189cd4",
              "queued_time": "2023-08-25T09:17:09.2360239Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": {
                "jobs": [],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "b3867972-38d7-400b-bd4e-75a3e99d299f",
              "state": "finished",
              "statement_id": 7
            },
            "text/plain": [
              "StatementMeta(b3867972-38d7-400b-bd4e-75a3e99d299f, 0, 7, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting auto_gptq\n",
            "  Downloading auto_gptq-0.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 962 kB/s eta 0:00:01\n",
            "\u001b[?25hCollecting accelerate>=0.19.0\n",
            "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 70.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting transformers>=4.31.0\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 71.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting peft\n",
            "  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 13.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[K     |████████████████████████████████| 519 kB 59.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: numpy in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from auto_gptq) (1.19.4)\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting safetensors\n",
            "  Downloading safetensors-0.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 78.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting torch>=1.13.0\n",
            "  Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 619.9 MB 14 kB/s s eta 0:00:012\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from accelerate>=0.19.0->auto_gptq) (5.4.1)\n",
            "Requirement already satisfied: psutil in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from accelerate>=0.19.0->auto_gptq) (5.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from accelerate>=0.19.0->auto_gptq) (21.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from packaging>=20.0->accelerate>=0.19.0->auto_gptq) (2.4.7)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 102.6 MB 9.5 kB/s  eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: networkx in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from torch>=1.13.0->auto_gptq) (2.5.1)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 557.1 MB 8.5 kB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 177.1 MB 21.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 15.9 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: filelock in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from torch>=1.13.0->auto_gptq) (3.8.0)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 173.2 MB 121.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from torch>=1.13.0->auto_gptq) (3.0.1)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 168.4 MB 109 kB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 317.1 MB 34 kB/s s eta 0:00:011\n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 83.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 53.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 63.2 MB 56.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 61.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.6 MB 809 kB/s  eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from torch>=1.13.0->auto_gptq) (3.10.0.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 83.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->auto_gptq) (49.6.0.post20210108)\n",
            "Requirement already satisfied: wheel in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->auto_gptq) (0.36.2)\n",
            "Collecting cmake\n",
            "  Downloading cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1 MB 70.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting lit\n",
            "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
            "\u001b[K     |████████████████████████████████| 153 kB 13.3 MB/s eta 0:00:01\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h    Preparing wheel metadata ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25hRequirement already satisfied: requests in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from transformers>=4.31.0->auto_gptq) (2.25.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 40.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from transformers>=4.31.0->auto_gptq) (4.61.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from transformers>=4.31.0->auto_gptq) (2021.7.6)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[K     |████████████████████████████████| 268 kB 44.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.31.0->auto_gptq) (2021.6.1)\n",
            "Requirement already satisfied: pandas in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from datasets->auto_gptq) (1.2.3)\n",
            "Collecting tqdm>=4.27\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 18.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 73.1 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pyarrow>=8.0.0\n",
            "  Downloading pyarrow-13.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 40.1 MB 67.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.15-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 83.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: dill<0.3.8,>=0.3.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from datasets->auto_gptq) (0.3.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[K     |████████████████████████████████| 194 kB 78.6 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from datasets->auto_gptq) (3.7.4.post0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp->datasets->auto_gptq) (21.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp->datasets->auto_gptq) (1.6.3)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp->datasets->auto_gptq) (4.0.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp->datasets->auto_gptq) (5.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from aiohttp->datasets->auto_gptq) (3.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests->transformers>=4.31.0->auto_gptq) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests->transformers>=4.31.0->auto_gptq) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests->transformers>=4.31.0->auto_gptq) (1.26.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from jinja2->torch>=1.13.0->auto_gptq) (2.0.1)\n",
            "Collecting dill<0.3.8,>=0.3.0\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 67.8 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: decorator<5,>=4.3 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from networkx->torch>=1.13.0->auto_gptq) (4.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from pandas->datasets->auto_gptq) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from pandas->datasets->auto_gptq) (2021.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets->auto_gptq) (1.16.0)\n",
            "Collecting mpmath>=0.19\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[K     |████████████████████████████████| 536 kB 81.3 MB/s eta 0:00:01\n",
            "\u001b[?25hBuilding wheels for collected packages: lit\n",
            "  Building wheel for lit (PEP 517) ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=b7597af19e8abdf83edae739980b8fcbf6bde079d0ffefe7ecf43a05355d3575\n",
            "  Stored in directory: /home/trusted-service-user/.cache/pip/wheels/05/ab/f1/0102fea49a41c753f0e79a1a4012417d5d7ef0f93224694472\n",
            "Successfully built lit\n",
            "Installing collected packages: nvidia-cublas-cu11, mpmath, lit, cmake, triton, tqdm, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-cusolver-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, fsspec, torch, tokenizers, safetensors, huggingface-hub, dill, xxhash, transformers, pyarrow, multiprocess, accelerate, rouge, peft, datasets, auto-gptq\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.61.2\n",
            "    Uninstalling tqdm-4.61.2:\n",
            "      Successfully uninstalled tqdm-4.61.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2021.6.1\n",
            "    Uninstalling fsspec-2021.6.1:\n",
            "      Successfully uninstalled fsspec-2021.6.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.8.1\n",
            "    Uninstalling torch-1.8.1:\n",
            "      Successfully uninstalled torch-1.8.1\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.9.1 requires torch==1.8.1, but you have torch 2.0.1 which is incompatible.\n",
            "azureml-dataset-runtime 1.34.0 requires pyarrow<4.0.0,>=0.17.0, but you have pyarrow 13.0.0 which is incompatible.\u001b[0m\n",
            "Successfully installed accelerate-0.22.0 auto-gptq-0.4.2 cmake-3.27.2 datasets-2.14.4 dill-0.3.7 fsspec-2023.6.0 huggingface-hub-0.16.4 lit-16.0.6 mpmath-1.3.0 multiprocess-0.70.15 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 peft-0.5.0 pyarrow-13.0.0 rouge-1.0.1 safetensors-0.3.3 sympy-1.12 tokenizers-0.13.3 torch-2.0.1 tqdm-4.66.1 transformers-4.32.0 triton-2.0.0 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install auto_gptq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (0.22.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from accelerate) (1.22.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: psutil in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.8)\n",
            "Requirement already satisfied: filelock in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.6.3)\n",
            "Requirement already satisfied: sympy in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/evilshadow/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1692957630059
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-08-25T10:00:30.2558496Z",
              "execution_start_time": "2023-08-25T09:56:00.3141745Z",
              "livy_statement_state": "available",
              "parent_msg_id": "acd2ca87-1b4f-4b69-b18a-f3cd585a452c",
              "queued_time": "2023-08-25T09:56:00.2353649Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": {
                "jobs": [],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "b3867972-38d7-400b-bd4e-75a3e99d299f",
              "state": "finished",
              "statement_id": 12
            },
            "text/plain": [
              "StatementMeta(b3867972-38d7-400b-bd4e-75a3e99d299f, 0, 12, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edf7676b-8910-46ca-990f-6b5484374cd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aca5f007-50de-4cdc-8e14-f42ec0ecd0f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.74M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03290bda-03f5-43e3-8e5d-2b042a1f31d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f4a01b4-2c1a-4351-8d25-9adffc5a375d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/941 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15d17940-bf44-4ad7-855c-137f09660523",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/configuration_RW.py:   0%|          | 0.00/2.51k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n",
            "- configuration_RW.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f63aa8d9-d6e8-4fd5-9f09-84fcbee2030d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)main/modelling_RW.py:   0%|          | 0.00/47.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:\n",
            "- modelling_RW.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0412a757-7b89-4a10-ba4d-971bbbeb0c7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)model.bin.index.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "127a3c9e-f9de-4e90-92ca-2b604d954146",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c1fea63-4eca-497c-82b2-ebf1d21ea35c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00001-of-00009.bin:   0%|          | 0.00/9.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab1e8baf-277a-4277-9349-814aae79c1f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00002-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "903a4261-b6e5-4006-a8e1-83680b3ba4b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00003-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "608c9184-ba40-4e3a-8489-8a65842d902e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00004-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "980f6492-b272-48fe-94dd-d0245571aa71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00005-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50c38d99-f3fc-4083-8136-a992df43e8d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00006-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 28] No space left on device",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_8194/3105424134.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m model = transformers.AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m )\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             )\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sharded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2785\u001b[0m             \u001b[0;31m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2786\u001b[0;31m             resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n\u001b[0m\u001b[1;32m   2787\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2788\u001b[0m                 \u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# Load from URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             cached_filename = cached_file(\n\u001b[0m\u001b[1;32m   1027\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                 \u001b[0mshard_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    429\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"downloading %s to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m             \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexpected_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexpected_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/tempfile.py\u001b[0m in \u001b[0;36mfunc_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0m_functools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0;31m# Avoid closing the file as long as the wrapper is alive,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;31m# see issue #18879.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import torch\n",
        "import transformers\n",
        "from huggingface_hub import hf_hub_download\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.bfloat16\n",
        "repo_id = \"jordiclive/falcon_lora_40b_ckpt_500_oasst_1\"\n",
        "base_model = \"tiiuae/falcon-40b\"\n",
        "\n",
        "# Model Loading\n",
        "def add_embeddings(model, embed_path, tokenizer):\n",
        "    old_embeddings = model.get_input_embeddings()\n",
        "    old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
        "    new_embeddings = torch.nn.Embedding(old_num_tokens, old_embedding_dim)\n",
        "    new_embeddings.to(old_embeddings.weight.device, dtype=old_embeddings.weight.dtype)\n",
        "    model._init_weights(new_embeddings)\n",
        "    embed_weights = torch.load(embed_path, map_location=old_embeddings.weight.device)\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    new_embeddings.weight.data[:vocab_size, :] = old_embeddings.weight.data[:vocab_size, :]\n",
        "    new_embeddings.weight.data[vocab_size : vocab_size + embed_weights.shape[0], :] = embed_weights.to(\n",
        "        new_embeddings.weight.dtype\n",
        "    ).to(new_embeddings.weight.device)\n",
        "    model.set_input_embeddings(new_embeddings)\n",
        "    model.tie_weights()\n",
        "\n",
        "\n",
        "\n",
        "def load_peft_model(model, peft_model_path, tokenizer):\n",
        "    embed_weights = hf_hub_download(peft_model_path, \"extra_embeddings.pt\")\n",
        "    model.resize_token_embeddings(tokenizer.vocab_size + torch.load(embed_weights).shape[0])\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.bos_token_id = tokenizer.bos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        model_id=peft_model_path,\n",
        "        torch_dtype=model.dtype,\n",
        "    )\n",
        "    model.eos_token_id = tokenizer.eos_token_id\n",
        "    add_embeddings(model, embed_weights, tokenizer)\n",
        "    return model\n",
        "\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(repo_id)\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    base_model, torch_dtype=dtype, trust_remote_code=True,\n",
        ")\n",
        "model = load_peft_model(model, repo_id, tokenizer)\n",
        "\n",
        "\n",
        "# device  configuration\n",
        "model = model.to(device)\n",
        "if dtype == torch.float16:\n",
        "    model = model.half()\n",
        "\n",
        "\n",
        "# Choose Generation parameters\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "\n",
        "def format_system_prompt(prompt, eos_token=\"</s>\"):\n",
        "    return \"{}{}{}{}\".format(\"<|prompter|>\", prompt, eos_token, \"<|assistant|>\")\n",
        "\n",
        "\n",
        "\n",
        "def generate(prompt, generation_config=generation_config, max_new_tokens=2048, device=device):\n",
        "    prompt = format_system_prompt(prompt)\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    retries = 3\n",
        "    retry_delay = 5  # seconds\n",
        "\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                generation_output = model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    generation_config=generation_config,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    eos_token_id=model.eos_token_id,\n",
        "                )\n",
        "            s = generation_output.sequences[0]\n",
        "            output = tokenizer.decode(s)\n",
        "            print(\"Text generated:\")\n",
        "            print(output)\n",
        "            return output\n",
        "        except (requests.RequestException, torch.TooManyRedirects) as e:\n",
        "            print(\"Error during generation. Retrying in {} seconds...\".format(retry_delay))\n",
        "            time.sleep(retry_delay)\n",
        "    else:\n",
        "        print(\"Max retries reached. Unable to generate text.\")\n",
        "        return None\n",
        "\n",
        "generate(\"What is a meme, and what's the history behind this word?\")\n",
        "generate(\"What's the Earth total population\")\n",
        "generate(\"Write a story about future of AI development\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": "2023-08-25T10:13:45.1198595Z",
              "livy_statement_state": "running",
              "parent_msg_id": "b18f3285-8650-4e8f-ba99-477a78e54f51",
              "queued_time": "2023-08-25T10:13:45.0527352Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": {
                "jobs": [],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "b3867972-38d7-400b-bd4e-75a3e99d299f",
              "state": "submitted",
              "statement_id": 13
            },
            "text/plain": [
              "StatementMeta(b3867972-38d7-400b-bd4e-75a3e99d299f, 0, 13, Submitted, Running)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65520b85-3a81-4831-bedf-1d3986a5010a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceafef12-9ee7-4fdd-a668-dc6080af6069",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00006-of-00009.bin:   0%|          | 0.00/9.51G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from huggingface_hub import hf_hub_download\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.bfloat16\n",
        "repo_id = \"jordiclive/falcon_lora_40b_ckpt_500_oasst_1\"\n",
        "base_model = \"tiiuae/falcon-40b\"\n",
        "\n",
        "# Model Loading\n",
        "def add_embeddings(model, embed_path, tokenizer):\n",
        "    old_embeddings = model.get_input_embeddings()\n",
        "    old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
        "    new_embeddings = torch.nn.Embedding(old_num_tokens, old_embedding_dim)\n",
        "    new_embeddings.to(old_embeddings.weight.device, dtype=old_embeddings.weight.dtype)\n",
        "    model._init_weights(new_embeddings)\n",
        "    embed_weights = torch.load(embed_path, map_location=old_embeddings.weight.device)\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    new_embeddings.weight.data[:vocab_size, :] = old_embeddings.weight.data[:vocab_size, :]\n",
        "    new_embeddings.weight.data[vocab_size : vocab_size + embed_weights.shape[0], :] = embed_weights.to(\n",
        "        new_embeddings.weight.dtype\n",
        "    ).to(new_embeddings.weight.device)\n",
        "    model.set_input_embeddings(new_embeddings)\n",
        "    model.tie_weights()\n",
        "\n",
        "\n",
        "\n",
        "def load_peft_model(model, peft_model_path, tokenizer):\n",
        "    embed_weights = hf_hub_download(peft_model_path, \"extra_embeddings.pt\")\n",
        "    model.resize_token_embeddings(tokenizer.vocab_size + torch.load(embed_weights).shape[0])\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.bos_token_id = tokenizer.bos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        model_id=peft_model_path,\n",
        "        torch_dtype=model.dtype,\n",
        "    )\n",
        "    model.eos_token_id = tokenizer.eos_token_id\n",
        "    add_embeddings(model, embed_weights, tokenizer)\n",
        "    return model\n",
        "\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(repo_id)\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    base_model, torch_dtype=dtype, trust_remote_code=True,\n",
        ")\n",
        "model = load_peft_model(model, repo_id, tokenizer)\n",
        "\n",
        "\n",
        "# device  configuration\n",
        "model = model.to(device)\n",
        "if dtype == torch.float16:\n",
        "    model = model.half()\n",
        "\n",
        "\n",
        "# Choose Generation parameters\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "\n",
        "def format_system_prompt(prompt, eos_token=\"</s>\"):\n",
        "    return \"{}{}{}{}\".format(\"<|prompter|>\", prompt, eos_token, \"<|assistant|>\")\n",
        "\n",
        "\n",
        "def generate(prompt, generation_config=generation_config, max_new_tokens=2048, device=device):\n",
        "    prompt = format_system_prompt(prompt)  # OpenAssistant Prompt Format expected\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=model.eos_token_id,\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s)\n",
        "    print(\"Text generated:\")\n",
        "    print(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "generate(\"What is a meme, and what's the history behind this word?\")\n",
        "generate(\"What's the Earth total population\")\n",
        "generate(\"Write a story about future of AI development\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
