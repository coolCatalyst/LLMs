{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afVSBpu6E-Lj"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers huggingface_hub peft\n"
      ],
      "metadata": {
        "id": "fO1q4XTRC9WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "0Sc36nBI_OdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto_gptq"
      ],
      "metadata": {
        "id": "9FgOeijGFy3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n"
      ],
      "metadata": {
        "id": "JQexhcNrGwFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from huggingface_hub import hf_hub_download\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.bfloat16\n",
        "repo_id = \"jordiclive/falcon_lora_40b_ckpt_500_oasst_1\"\n",
        "base_model = \"tiiuae/falcon-40b\"\n",
        "\n",
        "# Model Loading\n",
        "def add_embeddings(model, embed_path, tokenizer):\n",
        "    old_embeddings = model.get_input_embeddings()\n",
        "    old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
        "    new_embeddings = torch.nn.Embedding(old_num_tokens, old_embedding_dim)\n",
        "    new_embeddings.to(old_embeddings.weight.device, dtype=old_embeddings.weight.dtype)\n",
        "    model._init_weights(new_embeddings)\n",
        "    embed_weights = torch.load(embed_path, map_location=old_embeddings.weight.device)\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    new_embeddings.weight.data[:vocab_size, :] = old_embeddings.weight.data[:vocab_size, :]\n",
        "    new_embeddings.weight.data[vocab_size : vocab_size + embed_weights.shape[0], :] = embed_weights.to(\n",
        "        new_embeddings.weight.dtype\n",
        "    ).to(new_embeddings.weight.device)\n",
        "    model.set_input_embeddings(new_embeddings)\n",
        "    model.tie_weights()\n",
        "\n",
        "\n",
        "\n",
        "def load_peft_model(model, peft_model_path, tokenizer):\n",
        "    embed_weights = hf_hub_download(peft_model_path, \"extra_embeddings.pt\")\n",
        "    model.resize_token_embeddings(tokenizer.vocab_size + torch.load(embed_weights).shape[0])\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.bos_token_id = tokenizer.bos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        model_id=peft_model_path,\n",
        "        torch_dtype=model.dtype,\n",
        "    )\n",
        "    model.eos_token_id = tokenizer.eos_token_id\n",
        "    add_embeddings(model, embed_weights, tokenizer)\n",
        "    return model\n",
        "\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(repo_id)\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    base_model, torch_dtype=dtype, trust_remote_code=True,\n",
        ")\n",
        "model = load_peft_model(model, repo_id, tokenizer)\n",
        "\n",
        "\n",
        "# device  configuration\n",
        "model = model.to(device)\n",
        "if dtype == torch.float16:\n",
        "    model = model.half()\n",
        "\n",
        "\n",
        "# Choose Generation parameters\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "\n",
        "def format_system_prompt(prompt, eos_token=\"</s>\"):\n",
        "    return \"{}{}{}{}\".format(\"<|prompter|>\", prompt, eos_token, \"<|assistant|>\")\n",
        "\n",
        "\n",
        "def generate(prompt, generation_config=generation_config, max_new_tokens=2048, device=device):\n",
        "    prompt = format_system_prompt(prompt)  # OpenAssistant Prompt Format expected\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=model.eos_token_id,\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s)\n",
        "    print(\"Text generated:\")\n",
        "    print(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "generate(\"What is a meme, and what's the history behind this word?\")\n",
        "generate(\"What's the Earth total population\")\n",
        "generate(\"Write a story about future of AI development\")\n"
      ],
      "metadata": {
        "id": "7uyUO37YFhO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from huggingface_hub import hf_hub_download\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.bfloat16\n",
        "repo_id = \"jordiclive/falcon_lora_40b_ckpt_500_oasst_1\"\n",
        "base_model = \"tiiuae/falcon-40b\"\n",
        "\n",
        "# Model Loading\n",
        "def add_embeddings(model, embed_path, tokenizer):\n",
        "    old_embeddings = model.get_input_embeddings()\n",
        "    old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
        "    new_embeddings = torch.nn.Embedding(old_num_tokens, old_embedding_dim)\n",
        "    new_embeddings.to(old_embeddings.weight.device, dtype=old_embeddings.weight.dtype)\n",
        "    model._init_weights(new_embeddings)\n",
        "    embed_weights = torch.load(embed_path, map_location=old_embeddings.weight.device)\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    new_embeddings.weight.data[:vocab_size, :] = old_embeddings.weight.data[:vocab_size, :]\n",
        "    new_embeddings.weight.data[vocab_size : vocab_size + embed_weights.shape[0], :] = embed_weights.to(\n",
        "        new_embeddings.weight.dtype\n",
        "    ).to(new_embeddings.weight.device)\n",
        "    model.set_input_embeddings(new_embeddings)\n",
        "    model.tie_weights()\n",
        "\n",
        "\n",
        "\n",
        "def load_peft_model(model, peft_model_path, tokenizer):\n",
        "    embed_weights = hf_hub_download(peft_model_path, \"extra_embeddings.pt\")\n",
        "    model.resize_token_embeddings(tokenizer.vocab_size + torch.load(embed_weights).shape[0])\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.bos_token_id = tokenizer.bos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        model_id=peft_model_path,\n",
        "        torch_dtype=model.dtype,\n",
        "    )\n",
        "    model.eos_token_id = tokenizer.eos_token_id\n",
        "    add_embeddings(model, embed_weights, tokenizer)\n",
        "    return model\n",
        "\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(repo_id)\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    base_model, torch_dtype=dtype, trust_remote_code=True,\n",
        ")\n",
        "model = load_peft_model(model, repo_id, tokenizer)\n",
        "\n",
        "\n",
        "# device  configuration\n",
        "model = model.to(device)\n",
        "if dtype == torch.float16:\n",
        "    model = model.half()\n",
        "\n",
        "\n",
        "# Choose Generation parameters\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "\n",
        "def format_system_prompt(prompt, eos_token=\"</s>\"):\n",
        "    return \"{}{}{}{}\".format(\"<|prompter|>\", prompt, eos_token, \"<|assistant|>\")\n",
        "\n",
        "\n",
        "def generate(prompt, generation_config=generation_config, max_new_tokens=2048, device=device):\n",
        "    prompt = format_system_prompt(prompt)  # OpenAssistant Prompt Format expected\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=model.eos_token_id,\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s)\n",
        "    print(\"Text generated:\")\n",
        "    print(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "generate(\"What is a meme, and what's the history behind this word?\")\n",
        "generate(\"What's the Earth total population\")\n",
        "generate(\"Write a story about future of AI development\")\n"
      ],
      "metadata": {
        "id": "MDSSN98oDBZp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}