{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks2hjgNKR1kz"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "qOFo5S0lR4Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloomz-3b\", #tokenizer for all bloom models\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\")"
      ],
      "metadata": {
        "id": "T1VuPBCZR6hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model) #30 bloom block which we will target"
      ],
      "metadata": {
        "id": "dhQrQmPkR9JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#freezing weights and to ensure stability\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "  if param.ndim == 1:\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "PtqsYHWBR_VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],#the attention layer block to which we want to target\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)#the lorafied model\n",
        "#because of lora we have to train only 4925440 out of all params: 3007482880 isnt it crazy less than percent"
      ],
      "metadata": {
        "id": "0QYwUF6LSBy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "1AJhrjyUSHbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('FourthBrainGenAI/MarketMail-AI')\n",
        "print(dataset)\n",
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "Viv18yt8SH_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(product: str, description: str, marketing_email: str) -> str:\n",
        "  prompt = f\"### INSTRUCTION\\nBelow is a product and description, please write a marketing email for this product.\\n\\n### Product:\\n{product}\\n### Description:\\n{description}\\n\\n### Marketing Email:\\n{marketing_email}\"\n",
        "  return prompt\n",
        "\n",
        "mapped_dataset = dataset.map(lambda samples: tokenizer(generate_prompt(samples['product'], samples['description'], samples['marketing_email'])))"
      ],
      "metadata": {
        "id": "2SNByT2jSKq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=mapped_dataset[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        max_steps=100,\n",
        "        learning_rate=1e-3,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "lvJHlZVXSMxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()#make sure to pass the hubbing face token in write mode\n"
      ],
      "metadata": {
        "id": "M3tO2XMeSPyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"rohit-bloom-finetuned\"\n",
        "HUGGING_FACE_USER_NAME = \"rjindal\"\n",
        "model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", use_auth_token=True)"
      ],
      "metadata": {
        "id": "L7_3QJy9SYTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"rohit-bloom-finetuned\"\n",
        "HUGGING_FACE_USER_NAME = \"rjindal\"\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "dEk9eZ6HSbSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def make_inference(product, description):\n",
        "  batch = tokenizer(f\"### INSTRUCTION\\nBelow is a product and description, please write a marketing email for this product.\\n\\n### Product:\\n{product}\\n### Description:\\n{description}\\n\\n### Marketing Email:\\n\", return_tensors='pt')\n",
        "\n",
        "  with torch.cuda.amp.autocast():\n",
        "    output_tokens = model.generate(**batch, max_new_tokens=200)\n",
        "\n",
        "  display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
      ],
      "metadata": {
        "id": "vp8lWhpUSdnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "your_product_name_here = \"Iwatch\"\n",
        "your_product_description_here = \"mobile phone on your wrist!\"\n",
        "\n",
        "make_inference(your_product_name_here, your_product_description_here)"
      ],
      "metadata": {
        "id": "w8HjCsvQSeWy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}