{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import site\n",
        "\n",
        "def install_package_and_fix_import(package_name):\n",
        "    try:\n",
        "        # Create a virtual environment\n",
        "        subprocess.run([\"python3\", \"-m\", \"venv\", \"myenv\"])\n",
        "\n",
        "        # Activate the virtual environment\n",
        "        activate_script = \"myenv/bin/activate\"\n",
        "        subprocess.run([activate_script], shell=True)\n",
        "\n",
        "        # Install the package within the virtual environment\n",
        "        subprocess.run([\"pip\", \"install\", package_name])\n",
        "\n",
        "        # Get the site-packages directory of the virtual environment\n",
        "        venv_site_packages = site.getsitepackages()[0]\n",
        "\n",
        "        # Add the site-packages directory to sys.path\n",
        "        sys.path.append(venv_site_packages)\n",
        "\n",
        "        print(f\"Package {package_name} installed and sys.path updated successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "package_to_install = \"bitsandbytes\"\n",
        "install_package_and_fix_import(package_to_install)\n",
        "package_to_install = \"datasets\"\n",
        "install_package_and_fix_import(package_to_install)\n",
        "package_to_install = \"accelerate\"\n",
        "install_package_and_fix_import(package_to_install)\n",
        "package_to_install = \"loralib\"\n",
        "install_package_and_fix_import(package_to_install)\n",
        "package_to_install = \"transformers\"\n",
        "install_package_and_fix_import(package_to_install)\n",
        "package_to_install = \"git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git\"\n",
        "install_package_and_fix_import(package_to_install)\n",
        "package_to_install = \"torch\"\n",
        "install_package_and_fix_import(package_to_install)"
      ],
      "metadata": {
        "id": "xG6VopcC8uMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mFFRcxxeFo04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "package_to_install = \"os\"\n",
        "install_package_and_fix_import(package_to_install)"
      ],
      "metadata": {
        "id": "edKcHoB-DXYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "package_to_install = \"peft\"\n",
        "install_package_and_fix_import(package_to_install)"
      ],
      "metadata": {
        "id": "FG5pyfiWKJBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5bLzGkAJKKPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "metadata": {
        "id": "1_2Ak4PsL6hv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloomz-3b\", #tokenizer for all bloom models\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\")"
      ],
      "metadata": {
        "id": "oCCT1mXzEpZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model) #30 bloom block which we will target"
      ],
      "metadata": {
        "id": "bHZrW4RZEtu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#freezing weights and to ensure stability\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "  if param.ndim == 1:\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "YU07LEugGzJa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],#the attention layer block to which we want to target\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)#the lorafied model\n",
        "#because of lora we have to train only 4925440 out of all params: 3007482880 isnt it crazy less than percent"
      ],
      "metadata": {
        "id": "iRt_h_8bJvzM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "I0E37TtPJ0dK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('FourthBrainGenAI/MarketMail-AI')\n",
        "print(dataset)\n",
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "fgDSRmIDKXs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(product: str, description: str, marketing_email: str) -> str:\n",
        "  prompt = f\"### INSTRUCTION\\nBelow is a product and description, please write a marketing email for this product.\\n\\n### Product:\\n{product}\\n### Description:\\n{description}\\n\\n### Marketing Email:\\n{marketing_email}\"\n",
        "  return prompt\n",
        "\n",
        "mapped_dataset = dataset.map(lambda samples: tokenizer(generate_prompt(samples['product'], samples['description'], samples['marketing_email'])))\n",
        ""
      ],
      "metadata": {
        "id": "L6r1fOPtKbpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=mapped_dataset[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        max_steps=100,\n",
        "        learning_rate=1e-3,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()\n",
        ""
      ],
      "metadata": {
        "id": "cYhPVSatKfko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()#make sure to pass the hubbing face token in write mode"
      ],
      "metadata": {
        "id": "jEXjxfEjKkIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"rohit-bloom-finetuned\"\n",
        "HUGGING_FACE_USER_NAME = \"rjindal\"\n",
        "model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", use_auth_token=True)"
      ],
      "metadata": {
        "id": "1iEuZCdtK2C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"rohit-bloom-finetuned\"\n",
        "HUGGING_FACE_USER_NAME = \"rjindal\"\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "aNL-K3jUK7q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def make_inference(product, description):\n",
        "  batch = tokenizer(f\"### INSTRUCTION\\nBelow is a product and description, please write a marketing email for this product.\\n\\n### Product:\\n{product}\\n### Description:\\n{description}\\n\\n### Marketing Email:\\n\", return_tensors='pt')\n",
        "\n",
        "  with torch.cuda.amp.autocast():\n",
        "    output_tokens = model.generate(**batch, max_new_tokens=200)\n",
        "\n",
        "  display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))\n",
        ""
      ],
      "metadata": {
        "id": "w5A-iZG7LauW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "your_product_name_here = \"Iwatch\"\n",
        "your_product_description_here = \"mobile phone on your wrist!\"\n",
        "\n",
        "make_inference(your_product_name_here, your_product_description_here)"
      ],
      "metadata": {
        "id": "4O4rVpAFLb-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}